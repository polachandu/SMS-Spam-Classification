## Common Parameters in Pandas DataFrame
**groupby()**

Breaking down the df.groupby('target')['target'].agg('count').values()

- ![Screenshot 2025-01-03 at 8 33 02 AM](https://github.com/user-attachments/assets/18bd3edf-c80f-4ce9-99ad-ab8700cdb169)
- ![Screenshot 2025-01-03 at 8 33 32 AM](https://github.com/user-attachments/assets/632121da-b2fb-48b7-8baa-11d3a9ff292e)
- ![Screenshot 2025-01-03 at 8 34 08 AM](https://github.com/user-attachments/assets/5b33e8de-6d71-4bba-93ad-5bc0554dfc91)

**Is it necessary to remove text in Square Brackets and links while cleaning the data in NLP**

- ![Screenshot 2025-01-03 at 6 41 25 PM](https://github.com/user-attachments/assets/db269279-7644-4274-a2fb-797ced4dad57)
- ![Screenshot 2025-01-03 at 6 41 49 PM](https://github.com/user-attachments/assets/352c2701-1612-47be-a77c-34c80087c70c)

**Benefits of Tokenization**

Tokenization is the process of splitting text into smaller meaningful units, such as words, sentences, or sub-words. It’s a fundamental step in Natural Language Processing (NLP), enabling text data to be converted into a format understandable by machines.

- ![Screenshot 2025-01-05 at 8 01 45 PM](https://github.com/user-attachments/assets/6cfe6d3e-1cfe-4fe3-bcd1-6357cae623a5)

**Order of Tokenization, Stemming and CountVectorizer**
- ![Screenshot 2025-01-06 at 7 04 16 AM](https://github.com/user-attachments/assets/3fc6c3db-1e83-4d0a-bc44-01796d9e95fc)
- ![Screenshot 2025-01-06 at 7 08 46 AM](https://github.com/user-attachments/assets/1564a5e5-9bbc-4098-9df0-d469b2c72976)


