## Common Parameters in Pandas DataFrame
**groupby()**

Breaking down the df.groupby('target')['target'].agg('count').values()

1. ![Screenshot 2025-01-03 at 8 33 02 AM](https://github.com/user-attachments/assets/18bd3edf-c80f-4ce9-99ad-ab8700cdb169)
2. ![Screenshot 2025-01-03 at 8 33 32 AM](https://github.com/user-attachments/assets/632121da-b2fb-48b7-8baa-11d3a9ff292e)
3. ![Screenshot 2025-01-03 at 8 34 08 AM](https://github.com/user-attachments/assets/5b33e8de-6d71-4bba-93ad-5bc0554dfc91)

**Is it necessary to remove text in Square Brackets and links while cleaning the data in NLP**

1. ![Screenshot 2025-01-03 at 6 41 25 PM](https://github.com/user-attachments/assets/db269279-7644-4274-a2fb-797ced4dad57)
2. ![Screenshot 2025-01-03 at 6 41 49 PM](https://github.com/user-attachments/assets/352c2701-1612-47be-a77c-34c80087c70c)

**Benefits of Tokenization**

Tokenization is the process of splitting text into smaller meaningful units, such as words, sentences, or sub-words. It’s a fundamental step in Natural Language Processing (NLP), enabling text data to be converted into a format understandable by machines.

1. ![Screenshot 2025-01-05 at 8 01 45 PM](https://github.com/user-attachments/assets/6cfe6d3e-1cfe-4fe3-bcd1-6357cae623a5)

**Order of Tokenization, Stemming and CountVectorizer**
1. ![Screenshot 2025-01-06 at 7 04 16 AM](https://github.com/user-attachments/assets/3fc6c3db-1e83-4d0a-bc44-01796d9e95fc)
2. ![Screenshot 2025-01-06 at 7 08 46 AM](https://github.com/user-attachments/assets/1564a5e5-9bbc-4098-9df0-d469b2c72976)

**pad_sequences and text_to_sequences**

In Natural Language Processing (NLP) using TensorFlow/Keras, text_to_sequences and pad_sequences are commonly used for preprocessing textual data before feeding it into a neural network.

1. ![Screenshot 2025-01-07 at 8 13 50 AM](https://github.com/user-attachments/assets/9e91caf5-0c85-4be9-86bc-501cf74a6b87)
2. ![Screenshot 2025-01-07 at 8 18 56 AM](https://github.com/user-attachments/assets/d2c1c09f-5787-4c0b-a1a2-b4f120486999)
3. Always apply tokenization → sequence conversion → padding in that order.

**GloVe**
1. ![Screenshot 2025-01-07 at 7 20 41 PM](https://github.com/user-attachments/assets/b5e85878-012d-46ff-9e6c-f6770a577990)
2. ![Screenshot 2025-01-07 at 6 18 03 PM](https://github.com/user-attachments/assets/9a903bb4-215d-43bb-bdea-d1e8b2eccd61)
3. ![Screenshot 2025-01-07 at 6 18 40 PM](https://github.com/user-attachments/assets/6a7b933f-f473-485b-bdbf-216a9dba0ff6)
4. ![Screenshot 2025-01-07 at 6 19 05 PM](https://github.com/user-attachments/assets/9278acaa-bf01-4f7d-b589-c60d1b29f736)
5. ![Screenshot 2025-01-07 at 6 19 54 PM](https://github.com/user-attachments/assets/0ba4fdaf-9e26-47ea-8a8f-515daa0b54cd)




